{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2VA1IOI2z2Q"
   },
   "source": [
    "# ü§ó A Gentle Introduction to HuggingFace (HF)\n",
    "---\n",
    "HuggingFace provides you with a variety of pretrained models and\n",
    "functionalities to train/fine-tune these models and make inferences.\n",
    "\n",
    "Their [datasets](https://huggingface.co/docs/datasets/quickstart) library gives you access to many common NLP datasets. You can visualize these datasets on their [platform](https://huggingface.co/datasets) to get a sense of the data you would be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GQ3SRO_uAvYv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (2.10.1)\n",
      "Requirement already satisfied: transformers in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (4.27.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: aiohttp in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: packaging in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pandas in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/wa_ziqia/miniconda3/envs/deepl2023/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV1La_Z59UiU"
   },
   "source": [
    "## üå† Our Goal\n",
    "Our goal for this tutorial is to get familiar with the [transformers](https://huggingface.co/docs/transformers/index) library from HuggingFace and use a pretrained model to fine-tune it on a sequece classification task. More specifically we will fine-tune a [BERT](https://arxiv.org/pdf/1810.04805.pdf) model on the [Amazon Polarity](https://huggingface.co/datasets/amazon_polarity#data-instances) dataset.\n",
    "> The Amazon reviews dataset consists of reviews from amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review.\n",
    "\n",
    "> The Amazon reviews polarity dataset is constructed by taking review score 1 and 2 as negative, and 4 and 5 as positive. Samples of score 3 is ignored. Each class has 1,800,000 training samples and 200,000 testing samples.\n",
    "\n",
    "Since the dataset is quite large, we will be working with only a subset of this dataset throughout this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyytVSoo9Waq"
   },
   "source": [
    "## ü™ú Main Components\n",
    "The main components we would need to develop to realize our goal are:\n",
    "\n",
    "1. Load the data and make a dataset object for this task.\n",
    "2. Write a collate function/class to tokenize/transform/truncate batches of inputs.\n",
    "3. Make a custom model, which uses a pretrained model as its backbone and it is designed for our current task at hand.\n",
    "4. Write the training loop and train the model.\n",
    "\n",
    "> ‚ö†Ô∏è These steps constitues the basic building blocks to solve any other problem using HF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvJTeNSc9OR1"
   },
   "source": [
    "## üõí Loading data\n",
    "In this stage we will load the data from the `datasets` library. We will only load a small subset of the original dataset here in order to reduce the training time, but feel free to run this code on the full dataset on your own time and experiment with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WI2ee9F3dft",
    "outputId": "b455dd4c-3690-442a-e111-ecad3e899c8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset amazon_polarity (/home/wa_ziqia/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n",
      "Found cached dataset amazon_polarity (/home/wa_ziqia/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_train = load_dataset(\"amazon_polarity\", split=\"train[:1000]\")\n",
    "dataset_test = load_dataset(\"amazon_polarity\", split=\"test[:200]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9vWB3lbI3p2",
    "outputId": "ce62853b-91c5-453b-e3e9-062d2313c211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "title: Great CD\n",
      "content: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n",
      "label: 1\n",
      "------------------------------\n",
      "title: One of the best game music soundtracks - for a game I didn't really play\n",
      "content: Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, as there's not too many of those kinds of songs in my other video game soundtracks. I must admit that one of the songs (Life-A Distant Promise) has brought tears to my eyes on many occasions.My one complaint about this soundtrack is that they use guitar fretting effects in many of the songs, which I find distracting. But even if those weren't included I would still consider the collection worth it.\n",
      "label: 1\n",
      "------------------------------\n",
      "title: Batteries died within a year ...\n",
      "content: I bought this charger in Jul 2003 and it worked OK for a while. The design is nice and convenient. However, after about a year, the batteries would not hold a charge. Might as well just get alkaline disposables, or look elsewhere for a charger that comes with batteries that have better staying power.\n",
      "label: 0\n"
     ]
    }
   ],
   "source": [
    "#@title üîç Quick look at the data { run: \"auto\" }\n",
    "#@markdown Lets have quick look at a few samples as well as the label distributions in our train and test set.\n",
    "n_samples_to_see = 3 #@param {type: \"integer\"}\n",
    "for i in range(n_samples_to_see):\n",
    "  print(\"-\"*30)\n",
    "  print(\"title:\", dataset_test[i][\"title\"])\n",
    "  print(\"content:\", dataset_test[i][\"content\"])\n",
    "  print(\"label:\", dataset_test[i][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lgZxAErzE6PA"
   },
   "outputs": [],
   "source": [
    "def label_stats(ds):\n",
    "    negative = 0\n",
    "    positive = 0\n",
    "    for i in range(ds.num_rows):\n",
    "        if ds[i][\"label\"] == 1:\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIFY0vwJK5hx",
    "outputId": "8a573d2c-897f-4b4f-e8a1-aa9ee576e3aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "Set: train\n",
      "Positive samples: 462\n",
      "Negative samples: 538\n",
      "Percentage of overall positive samples: 46.2%\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "Set: test\n",
      "Positive samples: 109\n",
      "Negative samples: 91\n",
      "Percentage of overall positive samples: 54.5%\n"
     ]
    }
   ],
   "source": [
    "for i, ds in enumerate([dataset_train, dataset_test]):\n",
    "    positive, negative = label_stats(ds)\n",
    "    if i == 0:\n",
    "        str_indicator = \"train\"\n",
    "    else:\n",
    "        str_indicator = \"test\"\n",
    "    print(\"+-\" * 15)\n",
    "    print(\"Set:\", str_indicator)\n",
    "    print(f\"Positive samples: {positive}\\nNegative samples: {negative}\")\n",
    "    print(f\"Percentage of overall positive samples: {(positive*100.0)/(positive+negative)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80mbC9f8Q8au"
   },
   "source": [
    "## üß≤ Collate\n",
    "Collate is a function that is called on every batch of data prepared by the [dataloader](https://https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). Once we pass our dataset (e.g. `train_set`) to our dataloader, each batch will be a `list` of `dict` items. Therefore, this cannot be directed to the model. We need to perform the followings at this stage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzxwRDQFUtaG"
   },
   "source": [
    "### 1Ô∏è‚É£ Tokenize the `text`\n",
    "Tokenize the `text`portion of each sample (i.e. parsing the text to smaller chuncks). Tokenization can happen in many ways, traditionally this was done based the white spaces. With transformer-based models tokenization is performed based on the frequency of occurance of \"chunk of text\". This frequence can be learnt in many different ways, however the most common one is the [**wordpiece**](https://arxiv.org/pdf/1609.08144v2.pdf) model. \n",
    "> The wordpiece model is generated using a data-driven approach to maximize the language-model likelihood\n",
    "of the training data, given an evolving word definition. Given a training corpus and a number of desired\n",
    "tokens $D$, the optimization problem is to select $D$ wordpieces such that the resulting corpus is minimal in the\n",
    "number of wordpieces when segmented according to the chosen wordpiece model.\n",
    "\n",
    "Under this model:\n",
    "1. Not all things can be converted to tokens depending on the model. For example, most models have been pretrained without any knowledge of emojis. So their token will be `[UNK]`, which stands for unknown.\n",
    "2. Some words will be mapped to multiple tokens!\n",
    "3. Depending on the kind of model, your tokens may or may not respect capitalization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2Go_MrtGVR8g"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iu_rQoerVvsu",
    "outputId": "62731c30-338a-4bbf-b0ea-ce5aa6bf89a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'are',\n",
       " 'very',\n",
       " 'ju',\n",
       " '##bil',\n",
       " '##ant',\n",
       " 'to',\n",
       " 'demonstrate',\n",
       " 'to',\n",
       " 'you',\n",
       " 'the',\n",
       " '[UNK]',\n",
       " 'transformers',\n",
       " 'library',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title üîç Quick look at tokenization { run: \"auto\", vertical-output: true }\n",
    "input_sample = \"We are very jubilant to demonstrate to you the ü§ó Transformers library.\" #@param {type: \"string\"}\n",
    "tokenizer.tokenize(input_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEu6aqReXqp6"
   },
   "source": [
    "### 2Ô∏è‚É£ Encoding\n",
    "Once we have tokenized the text, we then need to convert these chuncks to numbers so we can feed them to our model. This conversion is basically a look-up in a dictionary **from `str` $\\to$ `int`**. The tokenizer object can also perform this work. While it does so it will also add the *special* tokens needed by the model to the encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpDGccrvYKnT",
    "outputId": "79f6a6f3-93bb-4306-f5f2-2ef7a5030b97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Token Encodings:\n",
      " [101, 2057, 2024, 2200, 18414, 14454, 4630, 2000, 10580, 2000, 2017, 1996, 100, 19081, 3075, 1012, 102]\n",
      "-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\n",
      "--> Token Encodings Decoded:\n",
      " [CLS] we are very jubilant to demonstrate to you the [UNK] transformers library. [SEP]\n"
     ]
    }
   ],
   "source": [
    "#@title üîç Quick look at token encoding { run: \"auto\"}\n",
    "input_sample = \"We are very jubilant to demonstrate to you the ü§ó Transformers library.\" #@param {type: \"string\"}\n",
    "print(\"--> Token Encodings:\\n\",tokenizer.encode(input_sample))\n",
    "print(\"-.\"*15)\n",
    "print(\"--> Token Encodings Decoded:\\n\",tokenizer.decode(tokenizer.encode(input_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DI8lFKZSZ2ZW"
   },
   "source": [
    "### 3Ô∏è‚É£ Truncate/Pad samples\n",
    "Since all the sample in the batch will not have the same sequence length, we would need to truncate the longers (i.e. the ones that exeed a predefined maximum length) and pad the shorter ones so we that we can equal length for all the samples in the batch. Once this is achieved, we would need to convert the result to `torch.Tensor`s and return. These tensors will then be retrieved from the [dataloader](https://https//pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SP31MsbHZxp5"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer: str, max_len: int) -> None:\n",
    "        self.tokenizer_name = tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
    "        texts = list(map(lambda batch_instance: batch_instance[\"title\"], batch))\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return dict(tokenized_inputs, **{\"labels\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4VaSpuyIjNqn"
   },
   "outputs": [],
   "source": [
    "#@title üßë‚Äçüç≥ Setting up the collate function { run: \"auto\" }\n",
    "tokenizer_name = \"distilbert-base-uncased\" #@param {type: \"string\"}\n",
    "sample_max_length = 64 #@param {type:\"slider\", min:32, max:512, step:1}\n",
    "collate = Collate(tokenizer=\"distilbert-base-uncased\", max_len=sample_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wG_15G_0dSyI"
   },
   "source": [
    "## ü§ñ Model\n",
    "Our model needs to classify an entire sequence of text. Once we feed an input sequence of length $k$ to a language model, it will output $k$ vectors. Now the question is which of these vectors or combition of these vectors should we use to classify the sequence?\n",
    "We will use the first toke, special token `[cls]` for these purposes. *Refer to the [BERT paper](https://arxiv.org/abs/1810.04805) for more information.*\n",
    "\n",
    "Since we have 2 classes (positive, and negative), this means we would need to make a classifier on top of the vector representations of the `[cls]` token. Our custom model will then look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "m3mLhXkqeopZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "class ReviewClassifier(torch.nn.Module):\n",
    "    def __init__(self, backbone: str, backbone_hidden_size: int, nb_classes: int):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.backbone_hidden_size = backbone_hidden_size\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        self.back_bone = AutoModel.from_pretrained(\n",
    "            self.backbone,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        self.classifier = torch.nn.Linear(self.backbone_hidden_size, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        back_bone_output = self.back_bone(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = back_bone_output[0]\n",
    "        pooled_output = hidden_states[:, 0]  # getting the [CLS] token\n",
    "\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HP58LrWUjFt4",
    "outputId": "8183cbb7-c906-4a8a-a71e-37edb2d046a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ReviewClassifier(backbone=\"distilbert-base-uncased\", backbone_hidden_size=768, nb_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmOKVEhOemuK"
   },
   "source": [
    "## üèì Training Loop\n",
    "In this section we will define the training loop to trian our model. Note that these model are sensative wrt the hyperparameters and it usually takes a while to find the right hyperparameters. The default hyperparameters should work fine for our test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaqWfQRdmp_T",
    "outputId": "7b63771a-337b-4e2d-dbfa-224ee47161dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Device selected: cuda\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import numpy as np\n",
    "\n",
    "print(f\"--> Device selected: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hhXqq3SknGTU"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: torch.nn.Module, training_data_loader: DataLoader, optimizer: torch.optim.Optimizer, logging_frequency: int\n",
    "):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    epoch_loss = 0\n",
    "    logging_loss = 0\n",
    "    for step, batch in enumerate(training_data_loader):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        logging_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % logging_frequency == 0:\n",
    "            print(f\"Training loss @ step {step+1}: {logging_loss/logging_frequency}\")\n",
    "            logging_loss = 0\n",
    "\n",
    "    return epoch_loss / len(training_data_loader)\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, test_data_loader: DataLoader, nb_classes: int):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    eval_loss = 0\n",
    "    correct_predictions = {i: 0 for i in range(nb_classes)}\n",
    "    total_predictions = {i: 0 for i in range(nb_classes)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(test_data_loader):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[0]\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            predictions = np.argmax(outputs[1].detach().cpu().numpy(), axis=1)\n",
    "            for target, prediction in zip(batch[\"labels\"].cpu().numpy(), predictions):\n",
    "                if target == prediction:\n",
    "                    correct_predictions[target] += 1\n",
    "                total_predictions[target] += 1\n",
    "\n",
    "    accuracy = (100.0 * sum(correct_predictions.values())) / sum(total_predictions.values())\n",
    "    return accuracy, eval_loss / len(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "muLjnpSpvXOx"
   },
   "outputs": [],
   "source": [
    "#@title üßë‚Äçüç≥ Setting hyperparameters for training { run: \"auto\" }\n",
    "nb_epoch = 3 #@param {type: \"slider\", min:1, max:10, step:1}\n",
    "batch_size = 64 #@param {type: \"integer\"}\n",
    "logging_frequency = 5 #@param {type: \"integer\"}\n",
    "learning_rate = 1e-5 #@param {type: \"number\"}\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# setting up the optimizer\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268,
     "referenced_widgets": [
      "48d33845788e40b7a5185f18b80bf0ae",
      "bdd3f08a72f74a47a0a4fa326ba98bba",
      "3dae6a5a03a44607a18909460d85b4d6",
      "7582bb5f909e45d78e248fc3c34a4535",
      "056b155d7aac48c09d6d97ea1ea3c6f7",
      "106716dec28142978e00f3f459a69214",
      "284dc766d41945828354e782e515319c",
      "b94e02d0a4d2493ba1206be86de1410f",
      "a4d95243125d4d8fa9ccc6db7896a8aa",
      "8b998d1186954872b81d097fdddd4336",
      "2be8a70acf324491aae37716eb6ccc2d"
     ]
    },
    "id": "RQhas_JxwVHS",
    "outputId": "ed466d10-f68f-4c7e-f2fe-4827ca8ee389"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b05f27ac4944e2ab0e458fe39656b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss @ step 5: 0.6864136815071106\n",
      "Training loss @ step 10: 0.6735731720924377\n",
      "Training loss @ step 15: 0.6486520171165466\n",
      "    Epoch: 1 Loss/Test: 0.6080581694841385, Loss/Test: 0.6677345559000969, Acc/Test: 73.5\n",
      "Training loss @ step 5: 0.5689218640327454\n",
      "Training loss @ step 10: 0.5336737990379333\n",
      "Training loss @ step 15: 0.4956661880016327\n",
      "    Epoch: 2 Loss/Test: 0.3880041316151619, Loss/Test: 0.5235507879406214, Acc/Test: 84.0\n",
      "Training loss @ step 5: 0.37964634895324706\n",
      "Training loss @ step 10: 0.39118672013282774\n",
      "Training loss @ step 15: 0.31929888725280764\n",
      "    Epoch: 3 Loss/Test: 0.561337485909462, Loss/Test: 0.37485682033002377, Acc/Test: 70.0\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "train_bar = tqdm(range(nb_epoch), desc=\"Epoch\")\n",
    "for e in train_bar:\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, logging_frequency)\n",
    "    eval_acc, eval_loss  = evaluate(model, test_loader, 2)\n",
    "    print(f\"    Epoch: {e+1} Loss/Test: {eval_loss}, Loss/Test: {train_loss}, Acc/Test: {eval_acc}\")\n",
    "    train_bar.set_postfix({\"Loss/Train\": train_loss, \"Loss/Test\": eval_loss, \"Acc/Test\": eval_acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyS-iVJJAskF"
   },
   "source": [
    "# üóÉÔ∏è Exercises\n",
    "It is suggested that you have look over the `tokenizer` class and its functionalities before attempting the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs4e1wmqBmGI"
   },
   "source": [
    "## 1Ô∏è‚É£ Predict with more context\n",
    "In the above training we only took advantage of the `title` of each review to predict its polarity.\n",
    "1. Investigate whether it would be useful to instead use the `content` of each review?\n",
    "2. Further investigate if it would be usefult to have both the `title` and `content` presented to model during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3391dfeb51b4510870af48bb31550d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss @ step 5: 0.7047205448150635\n",
      "Training loss @ step 10: 0.6284037232398987\n",
      "Training loss @ step 15: 0.498206752538681\n",
      "    Epoch: 1 Loss/Test: 0.5485259592533112, Loss/Test: 0.6075660232454538, Acc/Test: 77.0\n",
      "Training loss @ step 5: 0.5838162899017334\n",
      "Training loss @ step 10: 0.6462031006813049\n",
      "Training loss @ step 15: 0.6171675443649292\n",
      "    Epoch: 2 Loss/Test: 0.4981459751725197, Loss/Test: 0.6082155182957649, Acc/Test: 75.0\n",
      "Training loss @ step 5: 0.42882089614868163\n",
      "Training loss @ step 10: 0.39044376015663146\n",
      "Training loss @ step 15: 0.4203352272510529\n",
      "    Epoch: 3 Loss/Test: 0.6746558994054794, Loss/Test: 0.41266988404095173, Acc/Test: 74.5\n"
     ]
    }
   ],
   "source": [
    "# 1. Update the Collate class to use the \"content\" \n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer: str, max_len: int) -> None:\n",
    "        self.tokenizer_name = tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
    "        texts = list(map(lambda batch_instance: batch_instance[\"content\"], batch))\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return dict(tokenized_inputs, **{\"labels\": labels})\n",
    "\n",
    "tokenizer_name = \"distilbert-base-uncased\" #@param {type: \"string\"}\n",
    "sample_max_length = 64 #@param {type:\"slider\", min:32, max:512, step:1}\n",
    "collate = Collate(tokenizer=\"distilbert-base-uncased\", max_len=sample_max_length)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_bar = tqdm(range(nb_epoch), desc=\"Epoch\")\n",
    "for e in train_bar:\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, logging_frequency)\n",
    "    eval_acc, eval_loss  = evaluate(model, test_loader, 2)\n",
    "    print(f\"    Epoch: {e+1} Loss/Test: {eval_loss}, Loss/Test: {train_loss}, Acc/Test: {eval_acc}\")\n",
    "    train_bar.set_postfix({\"Loss/Train\": train_loss, \"Loss/Test\": eval_loss, \"Acc/Test\": eval_acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using content presents better and more stable performance during training. Content is more informative than title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed3a9c21329429ebab3cfbb50058b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss @ step 5: 0.423386812210083\n",
      "Training loss @ step 10: 0.35766669511795046\n",
      "Training loss @ step 15: 0.27586591243743896\n",
      "    Epoch: 1 Loss/Test: 0.331856369972229, Loss/Test: 0.34316360112279654, Acc/Test: 84.0\n",
      "Training loss @ step 5: 0.30392126441001893\n",
      "Training loss @ step 10: 0.38584625720977783\n",
      "Training loss @ step 15: 0.35124048590660095\n",
      "    Epoch: 2 Loss/Test: 0.30827657133340836, Loss/Test: 0.3360754083842039, Acc/Test: 85.0\n",
      "Training loss @ step 5: 0.19805218279361725\n",
      "Training loss @ step 10: 0.17479583621025085\n",
      "Training loss @ step 15: 0.20260158479213713\n",
      "    Epoch: 3 Loss/Test: 0.29394229874014854, Loss/Test: 0.19540339522063732, Acc/Test: 87.5\n"
     ]
    }
   ],
   "source": [
    "# 2. Update the Collate class to use both the title and content\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer: str, max_len: int) -> None:\n",
    "        self.tokenizer_name = tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
    "        texts = list(map(lambda batch_instance: batch_instance[\"title\"] + \" [SEP] \" + batch_instance[\"content\"], batch))\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return dict(tokenized_inputs, **{\"labels\": labels})\n",
    "\n",
    "tokenizer_name = \"distilbert-base-uncased\" #@param {type: \"string\"}\n",
    "sample_max_length = 64 #@param {type:\"slider\", min:32, max:512, step:1}\n",
    "collate = Collate(tokenizer=\"distilbert-base-uncased\", max_len=sample_max_length)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_bar = tqdm(range(nb_epoch), desc=\"Epoch\")\n",
    "for e in train_bar:\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, logging_frequency)\n",
    "    eval_acc, eval_loss  = evaluate(model, test_loader, 2)\n",
    "    print(f\"    Epoch: {e+1} Loss/Test: {eval_loss}, Loss/Test: {train_loss}, Acc/Test: {eval_acc}\")\n",
    "    train_bar.set_postfix({\"Loss/Train\": train_loss, \"Loss/Test\": eval_loss, \"Acc/Test\": eval_acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with both title and content shows the best performance compared with the first two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UdJVpMfDo1X"
   },
   "source": [
    "## 2Ô∏è‚É£ Frozen representations\n",
    "Modify the backbone so that we would only train the classifier layer, and the backbone stays frozen. How does the results compare to the unfrozen version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efb9a2ce4eb4df399bf5c80628e2433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss @ step 5: 0.1596461296081543\n",
      "Training loss @ step 10: 0.09291488490998745\n",
      "Training loss @ step 15: 0.15172291845083236\n",
      "    Epoch: 1 Loss/Test: 0.5576765118166804, Loss/Test: 0.13323528214823455, Acc/Test: 85.5\n",
      "Training loss @ step 5: 0.10412054732441903\n",
      "Training loss @ step 10: 0.10575225353240966\n",
      "Training loss @ step 15: 0.14210815355181694\n",
      "    Epoch: 2 Loss/Test: 0.5515907891094685, Loss/Test: 0.11911454517394304, Acc/Test: 85.5\n",
      "Training loss @ step 5: 0.08342054337263108\n",
      "Training loss @ step 10: 0.11610592901706696\n",
      "Training loss @ step 15: 0.12816433608531952\n",
      "    Epoch: 3 Loss/Test: 0.5461094807833433, Loss/Test: 0.11300338478758931, Acc/Test: 85.5\n"
     ]
    }
   ],
   "source": [
    "classifier_params = [param for name, param in model.named_parameters() if \"classifier\" in name]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": classifier_params,\n",
    "        \"weight_decay\": 0.0,\n",
    "    }\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_bar = tqdm(range(nb_epoch), desc=\"Epoch\")\n",
    "for e in train_bar:\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, logging_frequency)\n",
    "    eval_acc, eval_loss  = evaluate(model, test_loader, 2)\n",
    "    print(f\"    Epoch: {e+1} Loss/Test: {eval_loss}, Loss/Test: {train_loss}, Acc/Test: {eval_acc}\")\n",
    "    train_bar.set_postfix({\"Loss/Train\": train_loss, \"Loss/Test\": eval_loss, \"Acc/Test\": eval_acc})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen backbone results in faster convergence. And the performance is good compared with the first three epochs of fine-tuning the entire model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBXdWeRSD_ez"
   },
   "source": [
    "## 3Ô∏è‚É£ (Optional) Freeze then unfreeze\n",
    "It has empirically been shown that freezing the backbone for the first few steps of training and then unfreezing it produces better performing models. Modify the training code to have this option for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe141e7502bd44728fa7a5d4ceaf9c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss @ step 5: 0.06775848586112261\n",
      "Training loss @ step 10: 0.03402771819382906\n",
      "Training loss @ step 15: 0.05575862657278776\n",
      "    Epoch: 1 Loss/Test: 0.6363000720739365, Loss/Train: 0.05318336054915562, Acc/Test: 85.0\n",
      "Training loss @ step 5: 0.043672281038016084\n",
      "Training loss @ step 10: 0.025968672893941402\n",
      "Training loss @ step 15: 0.016677454486489295\n",
      "    Epoch: 2 Loss/Test: 0.5121978521347046, Loss/Train: 0.02863416718901135, Acc/Test: 88.5\n",
      "Training loss @ step 5: 0.020574381947517394\n",
      "Training loss @ step 10: 0.02387645998969674\n",
      "Training loss @ step 15: 0.0067906203679740425\n",
      "    Epoch: 3 Loss/Test: 0.5763185098767281, Loss/Train: 0.0278868559980765, Acc/Test: 86.0\n"
     ]
    }
   ],
   "source": [
    "def update_optimizer(optimizer, model, backbone_frozen):\n",
    "    classifier_params = [param for name, param in model.named_parameters() if \"classifier\" in name]\n",
    "    if backbone_frozen:\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": classifier_params,\n",
    "                \"weight_decay\": 0.0,\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        backbone_params = [param for name, param in model.named_parameters() if \"classifier\" not in name]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": classifier_params + backbone_params,\n",
    "                \"weight_decay\": 0.0,\n",
    "            }\n",
    "        ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n",
    "    return optimizer\n",
    "\n",
    "initial_frozen_epochs = 1\n",
    "\n",
    "train_bar = tqdm(range(nb_epoch), desc=\"Epoch\")\n",
    "for e in train_bar:\n",
    "    # Update the optimizer based on whether the backbone should be frozen or not\n",
    "    backbone_frozen = e < initial_frozen_epochs\n",
    "    optimizer = update_optimizer(optimizer, model, backbone_frozen)\n",
    "\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, logging_frequency)\n",
    "    eval_acc, eval_loss = evaluate(model, test_loader, 2)\n",
    "    print(f\"    Epoch: {e+1} Loss/Test: {eval_loss}, Loss/Train: {train_loss}, Acc/Test: {eval_acc}\")\n",
    "    train_bar.set_postfix({\"Loss/Train\": train_loss, \"Loss/Test\": eval_loss, \"Acc/Test\": eval_acc})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWzdHXQeEq4y"
   },
   "source": [
    "## 4Ô∏è‚É£ (Optional) Build an emotion aware AI\n",
    "Lets now put everything we learned to the test by building an agent with some emotion detection abilities. Use the [emotion dataset](https://huggingface.co/datasets/emotion) to train an [ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)-based model to detect the six basic emotions in our datasets. (anger, fear, joy, love, sadness, and surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f0ecf0d49a4476afd50cee06ab7e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2b6e7ea17d48589507e515c3c9c61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d10065eea8544e38da37540b8dcd3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset emotion/split to /home/wa_ziqia/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71d86add0744ec5b8591431692cc872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dd63e0df264f0c867ed8eaf5c02393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/592k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c8850e57704df09b6264e687814090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/74.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30434c0bea64e70b376a6f9f9a2743b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/74.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0022dda4d7c4096be854094bb45ecc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset emotion downloaded and prepared to /home/wa_ziqia/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f182bbe79ed04b448fe9955dd3519d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a65f6b17984132b3b537b5f28a2f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82a68faa9a1497e9ca3ac3965097dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)ve/main/spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ad7fcd8d1f4f408d2c28ebf6e47438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"emotion\")\n",
    "dataset_train = dataset[\"train\"]\n",
    "dataset_test = dataset[\"test\"]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "# Update the Collate function to handle the emotion dataset\n",
    "class EmotionCollate:\n",
    "    def __init__(self, tokenizer: str, max_len: int) -> None:\n",
    "        self.tokenizer_name = tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
    "        texts = list(map(lambda batch_instance: batch_instance[\"text\"], batch))\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return dict(tokenized_inputs, **{\"labels\": labels})\n",
    "\n",
    "collate = EmotionCollate(tokenizer=\"albert-base-v2\", max_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bd9310eadc4f5ab045b8f5ec636780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss @ step 5: 1.7215044498443604\n",
      "Training loss @ step 10: 1.6728541612625123\n",
      "Training loss @ step 15: 1.708013939857483\n",
      "Training loss @ step 20: 1.7007489204406738\n",
      "Training loss @ step 25: 1.691190814971924\n",
      "Training loss @ step 30: 1.68984375\n",
      "Training loss @ step 35: 1.7012216806411744\n",
      "Training loss @ step 40: 1.66433744430542\n",
      "Training loss @ step 45: 1.6535715818405152\n",
      "Training loss @ step 50: 1.6232777118682862\n",
      "Training loss @ step 55: 1.6377749919891358\n",
      "Training loss @ step 60: 1.6054049253463745\n",
      "Training loss @ step 65: 1.5997618675231933\n",
      "Training loss @ step 70: 1.672364354133606\n",
      "Training loss @ step 75: 1.6390195608139038\n",
      "Training loss @ step 80: 1.635919737815857\n",
      "Training loss @ step 85: 1.564455223083496\n",
      "Training loss @ step 90: 1.6188425064086913\n",
      "Training loss @ step 95: 1.5686997652053833\n",
      "Training loss @ step 100: 1.6247116804122925\n",
      "Training loss @ step 105: 1.6575784921646117\n",
      "Training loss @ step 110: 1.5630854606628417\n",
      "Training loss @ step 115: 1.5854912996292114\n",
      "Training loss @ step 120: 1.6166377544403077\n",
      "Training loss @ step 125: 1.568805742263794\n",
      "Training loss @ step 130: 1.605193567276001\n",
      "Training loss @ step 135: 1.5484853506088256\n",
      "Training loss @ step 140: 1.545970368385315\n",
      "Training loss @ step 145: 1.5776808738708497\n",
      "Training loss @ step 150: 1.5762733697891236\n",
      "Training loss @ step 155: 1.6047330379486084\n",
      "Training loss @ step 160: 1.5539275646209716\n",
      "Training loss @ step 165: 1.5842020988464356\n",
      "Training loss @ step 170: 1.581087017059326\n",
      "Training loss @ step 175: 1.5657620191574098\n",
      "Training loss @ step 180: 1.536041522026062\n",
      "Training loss @ step 185: 1.5870162487030028\n",
      "Training loss @ step 190: 1.5717156171798705\n",
      "Training loss @ step 195: 1.5976205587387085\n",
      "Training loss @ step 200: 1.6329406023025512\n",
      "Training loss @ step 205: 1.5853654146194458\n",
      "Training loss @ step 210: 1.5541209697723388\n",
      "Training loss @ step 215: 1.584219717979431\n",
      "Training loss @ step 220: 1.5551318883895875\n",
      "Training loss @ step 225: 1.6683475971221924\n",
      "Training loss @ step 230: 1.5444633960723877\n",
      "Training loss @ step 235: 1.5633499383926392\n",
      "Training loss @ step 240: 1.600359272956848\n",
      "Training loss @ step 245: 1.5286327600479126\n",
      "Training loss @ step 250: 1.4412809371948243\n",
      " Epoch: 1 Loss/Test: 1.5523383021354675, Loss/Train: 1.6055807905197144, Acc/Test: 40.5\n",
      "Training loss @ step 5: 1.6455963373184204\n",
      "Training loss @ step 10: 1.5976862907409668\n",
      "Training loss @ step 15: 1.5037878036499024\n",
      "Training loss @ step 20: 1.5463937520980835\n",
      "Training loss @ step 25: 1.6281051158905029\n",
      "Training loss @ step 30: 1.5289281606674194\n",
      "Training loss @ step 35: 1.5675542116165162\n",
      "Training loss @ step 40: 1.5937638759613038\n",
      "Training loss @ step 45: 1.5666862726211548\n",
      "Training loss @ step 50: 1.5548333883285523\n",
      "Training loss @ step 55: 1.550923800468445\n",
      "Training loss @ step 60: 1.6352694988250733\n",
      "Training loss @ step 65: 1.5746037006378173\n",
      "Training loss @ step 70: 1.5564049243927003\n",
      "Training loss @ step 75: 1.550331997871399\n",
      "Training loss @ step 80: 1.5547084093093873\n",
      "Training loss @ step 85: 1.5025143146514892\n",
      "Training loss @ step 90: 1.583439540863037\n",
      "Training loss @ step 95: 1.572393274307251\n",
      "Training loss @ step 100: 1.6065670728683472\n",
      "Training loss @ step 105: 1.5577210187911987\n",
      "Training loss @ step 110: 1.5621851205825805\n",
      "Training loss @ step 115: 1.5057318210601807\n",
      "Training loss @ step 120: 1.5937745571136475\n",
      "Training loss @ step 125: 1.5180290937423706\n",
      "Training loss @ step 130: 1.6300873517990113\n",
      "Training loss @ step 135: 1.5919705629348755\n",
      "Training loss @ step 140: 1.5809332132339478\n",
      "Training loss @ step 145: 1.565569519996643\n",
      "Training loss @ step 150: 1.6122603178024293\n",
      "Training loss @ step 155: 1.5370992422103882\n",
      "Training loss @ step 160: 1.4713426589965821\n",
      "Training loss @ step 165: 1.512699055671692\n",
      "Training loss @ step 170: 1.5612898349761963\n",
      "Training loss @ step 175: 1.523965549468994\n",
      "Training loss @ step 180: 1.5182973861694335\n",
      "Training loss @ step 185: 1.5658039808273316\n",
      "Training loss @ step 190: 1.5062865257263183\n",
      "Training loss @ step 195: 1.5207314491271973\n",
      "Training loss @ step 200: 1.5480863332748414\n",
      "Training loss @ step 205: 1.5448449373245239\n",
      "Training loss @ step 210: 1.5327778100967406\n",
      "Training loss @ step 215: 1.4936366558074952\n",
      "Training loss @ step 220: 1.5325276851654053\n",
      "Training loss @ step 225: 1.4675686359405518\n",
      "Training loss @ step 230: 1.470639443397522\n",
      "Training loss @ step 235: 1.5713453769683838\n",
      "Training loss @ step 240: 1.5744708061218262\n",
      "Training loss @ step 245: 1.4658097505569458\n",
      "Training loss @ step 250: 1.4848499059677125\n",
      " Epoch: 2 Loss/Test: 1.514723762869835, Loss/Train: 1.5508565468788147, Acc/Test: 38.95\n",
      "Training loss @ step 5: 1.5025535345077514\n",
      "Training loss @ step 10: 1.5384912252426148\n",
      "Training loss @ step 15: 1.5607666969299316\n",
      "Training loss @ step 20: 1.5494430541992188\n",
      "Training loss @ step 25: 1.5014452934265137\n",
      "Training loss @ step 30: 1.49620463848114\n",
      "Training loss @ step 35: 1.495279598236084\n",
      "Training loss @ step 40: 1.5526311159133912\n",
      "Training loss @ step 45: 1.5129241466522216\n",
      "Training loss @ step 50: 1.487498927116394\n",
      "Training loss @ step 55: 1.5020352602005005\n",
      "Training loss @ step 60: 1.5494057178497314\n",
      "Training loss @ step 65: 1.5474505424499512\n",
      "Training loss @ step 70: 1.4782512187957764\n",
      "Training loss @ step 75: 1.5287511825561524\n",
      "Training loss @ step 80: 1.5503345489501954\n",
      "Training loss @ step 85: 1.4978762865066528\n",
      "Training loss @ step 90: 1.5505949974060058\n",
      "Training loss @ step 95: 1.5740411281585693\n",
      "Training loss @ step 100: 1.5102305173873902\n",
      "Training loss @ step 105: 1.469995641708374\n",
      "Training loss @ step 110: 1.5425589084625244\n",
      "Training loss @ step 115: 1.4792234420776367\n",
      "Training loss @ step 120: 1.500689959526062\n",
      "Training loss @ step 125: 1.5141486406326294\n",
      "Training loss @ step 130: 1.432607316970825\n",
      "Training loss @ step 135: 1.6038545846939087\n",
      "Training loss @ step 140: 1.4770460367202758\n",
      "Training loss @ step 145: 1.4397047996520995\n",
      "Training loss @ step 150: 1.4758578538894653\n",
      "Training loss @ step 155: 1.5662209749221803\n",
      "Training loss @ step 160: 1.516641092300415\n",
      "Training loss @ step 165: 1.5590457916259766\n",
      "Training loss @ step 170: 1.5390382766723634\n",
      "Training loss @ step 175: 1.4739015579223633\n",
      "Training loss @ step 180: 1.5877967596054077\n",
      "Training loss @ step 185: 1.5727082014083862\n",
      "Training loss @ step 190: 1.5541956663131713\n",
      "Training loss @ step 195: 1.5434831619262694\n",
      "Training loss @ step 200: 1.5289115905761719\n",
      "Training loss @ step 205: 1.4740438222885133\n",
      "Training loss @ step 210: 1.4742789030075074\n",
      "Training loss @ step 215: 1.507567572593689\n",
      "Training loss @ step 220: 1.6009572505950929\n",
      "Training loss @ step 225: 1.4586663007736207\n",
      "Training loss @ step 230: 1.5786720991134644\n",
      "Training loss @ step 235: 1.5728407144546508\n",
      "Training loss @ step 240: 1.4661128759384154\n",
      "Training loss @ step 245: 1.4968255519866944\n",
      "Training loss @ step 250: 1.5157749652862549\n",
      " Epoch: 3 Loss/Test: 1.4881233870983124, Loss/Train: 1.520191598892212, Acc/Test: 39.65\n",
      "Training loss @ step 5: 1.5086355924606323\n",
      "Training loss @ step 10: 1.5153269529342652\n",
      "Training loss @ step 15: 1.4878267526626587\n",
      "Training loss @ step 20: 1.4745744466781616\n",
      "Training loss @ step 25: 1.501077127456665\n",
      "Training loss @ step 30: 1.5202488422393798\n",
      "Training loss @ step 35: 1.4545573234558105\n",
      "Training loss @ step 40: 1.4615532398223876\n",
      "Training loss @ step 45: 1.5138927698135376\n",
      "Training loss @ step 50: 1.5435882329940795\n",
      "Training loss @ step 55: 1.5030220746994019\n",
      "Training loss @ step 60: 1.560760259628296\n",
      "Training loss @ step 65: 1.5162882089614869\n",
      "Training loss @ step 70: 1.4862077474594115\n",
      "Training loss @ step 75: 1.4847218990325928\n",
      "Training loss @ step 80: 1.5030912160873413\n",
      "Training loss @ step 85: 1.4812950372695923\n",
      "Training loss @ step 90: 1.5721893310546875\n",
      "Training loss @ step 95: 1.4571950674057006\n",
      "Training loss @ step 100: 1.436559009552002\n",
      "Training loss @ step 105: 1.468324565887451\n",
      "Training loss @ step 110: 1.5171959400177002\n",
      "Training loss @ step 115: 1.442266583442688\n",
      "Training loss @ step 120: 1.4306885719299316\n",
      "Training loss @ step 125: 1.5396093368530273\n",
      "Training loss @ step 130: 1.526958131790161\n",
      "Training loss @ step 135: 1.5295637607574464\n",
      "Training loss @ step 140: 1.460082721710205\n",
      "Training loss @ step 145: 1.566672444343567\n",
      "Training loss @ step 150: 1.508916974067688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss @ step 155: 1.447930884361267\n",
      "Training loss @ step 160: 1.5458407402038574\n",
      "Training loss @ step 165: 1.5125418186187745\n",
      "Training loss @ step 170: 1.4524744272232055\n",
      "Training loss @ step 175: 1.532536220550537\n",
      "Training loss @ step 180: 1.4891494035720825\n",
      "Training loss @ step 185: 1.4802096605300903\n",
      "Training loss @ step 190: 1.4525571584701538\n",
      "Training loss @ step 195: 1.5307323455810546\n",
      "Training loss @ step 200: 1.5394468784332276\n",
      "Training loss @ step 205: 1.4495439529418945\n",
      "Training loss @ step 210: 1.4482901573181153\n",
      "Training loss @ step 215: 1.5393302679061889\n",
      "Training loss @ step 220: 1.4884023427963258\n",
      "Training loss @ step 225: 1.5147291660308837\n",
      "Training loss @ step 230: 1.4018651247024536\n",
      "Training loss @ step 235: 1.4867803812026978\n",
      "Training loss @ step 240: 1.4492143392562866\n",
      "Training loss @ step 245: 1.4761451482772827\n",
      "Training loss @ step 250: 1.4701338291168213\n",
      " Epoch: 4 Loss/Test: 1.4892820194363594, Loss/Train: 1.4936148881912232, Acc/Test: 37.75\n",
      "Training loss @ step 5: 1.4425822257995606\n",
      "Training loss @ step 10: 1.4574259281158448\n",
      "Training loss @ step 15: 1.4636321544647217\n",
      "Training loss @ step 20: 1.4392324924468993\n",
      "Training loss @ step 25: 1.4606940507888795\n",
      "Training loss @ step 30: 1.4845847606658935\n",
      "Training loss @ step 35: 1.4754969120025634\n",
      "Training loss @ step 40: 1.501175832748413\n",
      "Training loss @ step 45: 1.486658477783203\n",
      "Training loss @ step 50: 1.497943925857544\n",
      "Training loss @ step 55: 1.4815522193908692\n",
      "Training loss @ step 60: 1.5093823194503784\n",
      "Training loss @ step 65: 1.4148756265640259\n",
      "Training loss @ step 70: 1.4436028242111205\n",
      "Training loss @ step 75: 1.4206866025924683\n",
      "Training loss @ step 80: 1.4867148637771606\n",
      "Training loss @ step 85: 1.5396081924438476\n",
      "Training loss @ step 90: 1.5487081289291382\n",
      "Training loss @ step 95: 1.4921606063842774\n",
      "Training loss @ step 100: 1.456519079208374\n",
      "Training loss @ step 105: 1.5167119979858399\n",
      "Training loss @ step 110: 1.504753541946411\n",
      "Training loss @ step 115: 1.42199604511261\n",
      "Training loss @ step 120: 1.5402597427368163\n",
      "Training loss @ step 125: 1.5350120306015014\n",
      "Training loss @ step 130: 1.5210614919662475\n",
      "Training loss @ step 135: 1.521850299835205\n",
      "Training loss @ step 140: 1.4877012014389037\n",
      "Training loss @ step 145: 1.5242607116699218\n",
      "Training loss @ step 150: 1.5570303201675415\n",
      "Training loss @ step 155: 1.5494842052459716\n",
      "Training loss @ step 160: 1.52783043384552\n",
      "Training loss @ step 165: 1.5176272630691527\n",
      "Training loss @ step 170: 1.4935159921646117\n",
      "Training loss @ step 175: 1.5014246940612792\n",
      "Training loss @ step 180: 1.4504627227783202\n",
      "Training loss @ step 185: 1.4428431749343873\n",
      "Training loss @ step 190: 1.4891221284866334\n",
      "Training loss @ step 195: 1.5170034885406494\n",
      "Training loss @ step 200: 1.43388831615448\n",
      "Training loss @ step 205: 1.4969878196716309\n",
      "Training loss @ step 210: 1.4526869058609009\n",
      "Training loss @ step 215: 1.5029356002807617\n",
      "Training loss @ step 220: 1.4714970350265504\n",
      "Training loss @ step 225: 1.4887535095214843\n",
      "Training loss @ step 230: 1.4887449026107789\n",
      "Training loss @ step 235: 1.5000856637954711\n",
      "Training loss @ step 240: 1.4765613317489623\n",
      "Training loss @ step 245: 1.4859441280364991\n",
      "Training loss @ step 250: 1.5720912218093872\n",
      " Epoch: 5 Loss/Test: 1.495351005345583, Loss/Train: 1.4898673028945923, Acc/Test: 35.95\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertModel\n",
    "\n",
    "\n",
    "model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "class EmotionClassifier(torch.nn.Module):\n",
    "    def __init__(self, backbone: str, backbone_hidden_size: int, nb_classes: int):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.backbone_hidden_size = backbone_hidden_size\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        self.back_bone = AlbertModel.from_pretrained(\n",
    "            self.backbone,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        self.classifier = torch.nn.Linear(self.backbone_hidden_size, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        back_bone_output = self.back_bone(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = back_bone_output[0]\n",
    "        pooled_output = hidden_states[:, 0]  # getting the [CLS] token\n",
    "\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "model = EmotionClassifier(backbone=\"albert-base-v2\", backbone_hidden_size=768, nb_classes=6)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "model.to(device)\n",
    "nb_epoch = 5\n",
    "classifier_params = [param for name, param in model.named_parameters() if \"classifier\" in name]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": classifier_params,\n",
    "        \"weight_decay\": 0.0,\n",
    "    }\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n",
    "train_bar = tqdm(range(nb_epoch), desc=\"Epoch\")\n",
    "for e in train_bar:\n",
    "    # Update the optimizer based on whether the backbone should be frozen or not\n",
    "#     backbone_frozen = e < initial_frozen_epochs\n",
    "#     optimizer = update_optimizer(optimizer, model, backbone_frozen)\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, logging_frequency)\n",
    "    eval_acc, eval_loss = evaluate(model, test_loader, 6) # There are 6 classes in the emotion dataset\n",
    "    print(f\" Epoch: {e+1} Loss/Test: {eval_loss}, Loss/Train: {train_loss}, Acc/Test: {eval_acc}\")\n",
    "    train_bar.set_postfix({\"Loss/Train\": train_loss, \"Loss/Test\": eval_loss, \"Acc/Test\": eval_acc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion: fear\n"
     ]
    }
   ],
   "source": [
    "def predict_emotion(model, tokenizer, text: str):\n",
    "    emotions = [\"anger\", \"fear\", \"joy\", \"love\", \"sadness\", \"surprise\"]\n",
    "\n",
    "    tokenized_input = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "    input_ids = tokenized_input[\"input_ids\"].to(device)\n",
    "    attention_mask = tokenized_input[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probabilities = torch.softmax(logits, dim=-1).cpu().numpy().flatten()\n",
    "        predicted_emotion = emotions[np.argmax(probabilities)]\n",
    "\n",
    "    return predicted_emotion, probabilities\n",
    "\n",
    "\n",
    "text = \"I am fear.\"\n",
    "predicted_emotion, probabilities = predict_emotion(model, tokenizer, text)\n",
    "print(f\"Predicted emotion: {predicted_emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "056b155d7aac48c09d6d97ea1ea3c6f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2be8a70acf324491aae37716eb6ccc2d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8b998d1186954872b81d097fdddd4336",
      "value": " 3/3 [00:14&lt;00:00,  4.89s/it, Loss/Train=0.395, Loss/Test=0.353, Acc/Test=84.5]"
     }
    },
    "106716dec28142978e00f3f459a69214": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "284dc766d41945828354e782e515319c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2be8a70acf324491aae37716eb6ccc2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3dae6a5a03a44607a18909460d85b4d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_284dc766d41945828354e782e515319c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_106716dec28142978e00f3f459a69214",
      "value": "Epoch: 100%"
     }
    },
    "48d33845788e40b7a5185f18b80bf0ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3dae6a5a03a44607a18909460d85b4d6",
       "IPY_MODEL_7582bb5f909e45d78e248fc3c34a4535",
       "IPY_MODEL_056b155d7aac48c09d6d97ea1ea3c6f7"
      ],
      "layout": "IPY_MODEL_bdd3f08a72f74a47a0a4fa326ba98bba"
     }
    },
    "7582bb5f909e45d78e248fc3c34a4535": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4d95243125d4d8fa9ccc6db7896a8aa",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b94e02d0a4d2493ba1206be86de1410f",
      "value": 3
     }
    },
    "8b998d1186954872b81d097fdddd4336": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a4d95243125d4d8fa9ccc6db7896a8aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b94e02d0a4d2493ba1206be86de1410f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bdd3f08a72f74a47a0a4fa326ba98bba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
