{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNtvb_zpN4H7"
   },
   "source": [
    "# ü¶ú NN-Based Language Model\n",
    "In this excercise we will run a basic RNN based language model and answer some questions about the code. It is advised to use GPU to run the code. First run the code then answer the questions below that require modifying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CTOJyYyujICY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-tutorial'...\n",
      "remote: Enumerating objects: 917, done.\u001b[K\n",
      "remote: Total 917 (delta 0), reused 0 (delta 0), pack-reused 917\u001b[K\n",
      "Receiving objects: 100% (917/917), 12.80 MiB | 953.00 KiB/s, done.\n",
      "Resolving deltas: 100% (490/490), done.\n",
      "/home/wa_ziqia/Documents/assignments/COMP691/assignment2/pytorch-tutorial/tutorials/02-intermediate/language_model\n",
      "--> Device selected: cuda\n"
     ]
    }
   ],
   "source": [
    "#@title üßÆ Imports & Hyperparameter Setup\n",
    "#@markdown Feel free to experiment with the following hyperparameters at your\n",
    "#@markdown leasure. For the purpose of this assignment, leave the default values\n",
    "#@markdown and run the code with these suggested values.\n",
    "# Some part of the code was referenced from below.\n",
    "# https://github.com/pytorch/examples/tree/master/word_language_model \n",
    "# https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model\n",
    "\n",
    "! git clone https://github.com/yunjey/pytorch-tutorial/\n",
    "%cd pytorch-tutorial/tutorials/02-intermediate/language_model/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "embed_size = 128 #@param {type:\"number\"}\n",
    "hidden_size = 1024 #@param {type:\"number\"}\n",
    "num_layers = 1 #@param {type:\"number\"}\n",
    "num_epochs = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "batch_size = 20 #@param {type:\"number\"}\n",
    "seq_length = 30 #@param {type:\"number\"}\n",
    "learning_rate = 0.002 #@param {type:\"number\"}\n",
    "#@markdown Number of words to be sampled ‚¨áÔ∏è\n",
    "num_samples = 50 #@param {type:\"number\"}  \n",
    "\n",
    "print(f\"--> Device selected: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tzj73P_QeBEA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vcoabulary size: 10000\n",
      "Number of batches: 1549\n"
     ]
    }
   ],
   "source": [
    "from data_utils import Dictionary, Corpus\n",
    "\n",
    "# Load \"Penn Treebank\" dataset\n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data('data/train.txt', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length\n",
    "\n",
    "print(f\"Vcoabulary size: {vocab_size}\")\n",
    "print(f\"Number of batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKzalmp8dndK"
   },
   "source": [
    "## ü§ñ Model Definition\n",
    "As you can see below, this model stacks `num_layers` many [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) units vertically to construct our basic RNN-based language model. The diagram below shows a pictorial representation of the model in its simplest form (i.e `num_layers`=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QZTjM5fQri35"
   },
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_fjTZ6wdpae"
   },
   "source": [
    "## üèì Training\n",
    "In this section we will train our model, this should take a couple of minutes! Be patient üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "DsaIIUUHjQ5n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step[0/1549], Loss: 9.2125, Perplexity: 10021.20\n",
      "Epoch [1/5], Step[100/1549], Loss: 5.9930, Perplexity: 400.62\n",
      "Epoch [1/5], Step[200/1549], Loss: 5.9335, Perplexity: 377.48\n",
      "Epoch [1/5], Step[300/1549], Loss: 5.7679, Perplexity: 319.86\n",
      "Epoch [1/5], Step[400/1549], Loss: 5.6859, Perplexity: 294.68\n",
      "Epoch [1/5], Step[500/1549], Loss: 5.1088, Perplexity: 165.47\n",
      "Epoch [1/5], Step[600/1549], Loss: 5.1871, Perplexity: 178.95\n",
      "Epoch [1/5], Step[700/1549], Loss: 5.3405, Perplexity: 208.62\n",
      "Epoch [1/5], Step[800/1549], Loss: 5.1862, Perplexity: 178.80\n",
      "Epoch [1/5], Step[900/1549], Loss: 5.0756, Perplexity: 160.07\n",
      "Epoch [1/5], Step[1000/1549], Loss: 5.1047, Perplexity: 164.79\n",
      "Epoch [1/5], Step[1100/1549], Loss: 5.3735, Perplexity: 215.62\n",
      "Epoch [1/5], Step[1200/1549], Loss: 5.1934, Perplexity: 180.07\n",
      "Epoch [1/5], Step[1300/1549], Loss: 5.0448, Perplexity: 155.22\n",
      "Epoch [1/5], Step[1400/1549], Loss: 4.8532, Perplexity: 128.15\n",
      "Epoch [1/5], Step[1500/1549], Loss: 5.1126, Perplexity: 166.11\n",
      "Epoch [2/5], Step[0/1549], Loss: 5.4353, Perplexity: 229.37\n",
      "Epoch [2/5], Step[100/1549], Loss: 4.5311, Perplexity: 92.86\n",
      "Epoch [2/5], Step[200/1549], Loss: 4.6928, Perplexity: 109.16\n",
      "Epoch [2/5], Step[300/1549], Loss: 4.6528, Perplexity: 104.87\n",
      "Epoch [2/5], Step[400/1549], Loss: 4.5334, Perplexity: 93.07\n",
      "Epoch [2/5], Step[500/1549], Loss: 4.1462, Perplexity: 63.19\n",
      "Epoch [2/5], Step[600/1549], Loss: 4.4488, Perplexity: 85.53\n",
      "Epoch [2/5], Step[700/1549], Loss: 4.3758, Perplexity: 79.51\n",
      "Epoch [2/5], Step[800/1549], Loss: 4.4419, Perplexity: 84.94\n",
      "Epoch [2/5], Step[900/1549], Loss: 4.1842, Perplexity: 65.64\n",
      "Epoch [2/5], Step[1000/1549], Loss: 4.2938, Perplexity: 73.24\n",
      "Epoch [2/5], Step[1100/1549], Loss: 4.5950, Perplexity: 98.99\n",
      "Epoch [2/5], Step[1200/1549], Loss: 4.4834, Perplexity: 88.54\n",
      "Epoch [2/5], Step[1300/1549], Loss: 4.2096, Perplexity: 67.33\n",
      "Epoch [2/5], Step[1400/1549], Loss: 3.9488, Perplexity: 51.88\n",
      "Epoch [2/5], Step[1500/1549], Loss: 4.2778, Perplexity: 72.08\n",
      "Epoch [3/5], Step[0/1549], Loss: 6.4186, Perplexity: 613.12\n",
      "Epoch [3/5], Step[100/1549], Loss: 3.8480, Perplexity: 46.90\n",
      "Epoch [3/5], Step[200/1549], Loss: 3.9550, Perplexity: 52.20\n",
      "Epoch [3/5], Step[300/1549], Loss: 3.9774, Perplexity: 53.38\n",
      "Epoch [3/5], Step[400/1549], Loss: 3.8468, Perplexity: 46.84\n",
      "Epoch [3/5], Step[500/1549], Loss: 3.3904, Perplexity: 29.68\n",
      "Epoch [3/5], Step[600/1549], Loss: 3.8905, Perplexity: 48.93\n",
      "Epoch [3/5], Step[700/1549], Loss: 3.6955, Perplexity: 40.26\n",
      "Epoch [3/5], Step[800/1549], Loss: 3.7929, Perplexity: 44.39\n",
      "Epoch [3/5], Step[900/1549], Loss: 3.4134, Perplexity: 30.37\n",
      "Epoch [3/5], Step[1000/1549], Loss: 3.6280, Perplexity: 37.64\n",
      "Epoch [3/5], Step[1100/1549], Loss: 3.8570, Perplexity: 47.32\n",
      "Epoch [3/5], Step[1200/1549], Loss: 3.7760, Perplexity: 43.64\n",
      "Epoch [3/5], Step[1300/1549], Loss: 3.4525, Perplexity: 31.58\n",
      "Epoch [3/5], Step[1400/1549], Loss: 3.1765, Perplexity: 23.96\n",
      "Epoch [3/5], Step[1500/1549], Loss: 3.5267, Perplexity: 34.01\n",
      "Epoch [4/5], Step[0/1549], Loss: 4.5772, Perplexity: 97.24\n",
      "Epoch [4/5], Step[100/1549], Loss: 3.2972, Perplexity: 27.04\n",
      "Epoch [4/5], Step[200/1549], Loss: 3.4941, Perplexity: 32.92\n",
      "Epoch [4/5], Step[300/1549], Loss: 3.4213, Perplexity: 30.61\n",
      "Epoch [4/5], Step[400/1549], Loss: 3.3724, Perplexity: 29.15\n",
      "Epoch [4/5], Step[500/1549], Loss: 2.8634, Perplexity: 17.52\n",
      "Epoch [4/5], Step[600/1549], Loss: 3.4233, Perplexity: 30.67\n",
      "Epoch [4/5], Step[700/1549], Loss: 3.1417, Perplexity: 23.14\n",
      "Epoch [4/5], Step[800/1549], Loss: 3.3011, Perplexity: 27.14\n",
      "Epoch [4/5], Step[900/1549], Loss: 2.9525, Perplexity: 19.15\n",
      "Epoch [4/5], Step[1000/1549], Loss: 3.1870, Perplexity: 24.22\n",
      "Epoch [4/5], Step[1100/1549], Loss: 3.3395, Perplexity: 28.20\n",
      "Epoch [4/5], Step[1200/1549], Loss: 3.3653, Perplexity: 28.94\n",
      "Epoch [4/5], Step[1300/1549], Loss: 2.9624, Perplexity: 19.34\n",
      "Epoch [4/5], Step[1400/1549], Loss: 2.7995, Perplexity: 16.44\n",
      "Epoch [4/5], Step[1500/1549], Loss: 3.0662, Perplexity: 21.46\n",
      "Epoch [5/5], Step[0/1549], Loss: 3.6857, Perplexity: 39.87\n",
      "Epoch [5/5], Step[100/1549], Loss: 3.0083, Perplexity: 20.25\n",
      "Epoch [5/5], Step[200/1549], Loss: 3.1410, Perplexity: 23.13\n",
      "Epoch [5/5], Step[300/1549], Loss: 3.0676, Perplexity: 21.49\n",
      "Epoch [5/5], Step[400/1549], Loss: 3.0267, Perplexity: 20.63\n",
      "Epoch [5/5], Step[500/1549], Loss: 2.5589, Perplexity: 12.92\n",
      "Epoch [5/5], Step[600/1549], Loss: 3.1300, Perplexity: 22.87\n",
      "Epoch [5/5], Step[700/1549], Loss: 2.8441, Perplexity: 17.19\n",
      "Epoch [5/5], Step[800/1549], Loss: 2.9708, Perplexity: 19.51\n",
      "Epoch [5/5], Step[900/1549], Loss: 2.6096, Perplexity: 13.59\n",
      "Epoch [5/5], Step[1000/1549], Loss: 2.8992, Perplexity: 18.16\n",
      "Epoch [5/5], Step[1100/1549], Loss: 3.0622, Perplexity: 21.37\n",
      "Epoch [5/5], Step[1200/1549], Loss: 3.0491, Perplexity: 21.10\n",
      "Epoch [5/5], Step[1300/1549], Loss: 2.6631, Perplexity: 14.34\n",
      "Epoch [5/5], Step[1400/1549], Loss: 2.4941, Perplexity: 12.11\n",
      "Epoch [5/5], Step[1500/1549], Loss: 2.7091, Perplexity: 15.02\n"
     ]
    }
   ],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states] \n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Vy9OJMEXRJs"
   },
   "source": [
    "# ü§î Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhis12qSX-ce"
   },
   "source": [
    "## 1Ô∏è‚É£ Q2.1 Detaching or not? (10 points)\n",
    "The above code implements a version of truncated backpropagation through time. The implementation only requires the `detach()` function (lines 7-9 of the cell) defined above the loop and used once inside the training loop.\n",
    "* Explain the implementation (compared to not using truncated backprop through time).\n",
    "* What does the `detach()` call here achieve? Draw a computational graph. You may choose to answer this question outside the notebook.\n",
    "* When using using line 7-9 we will typically observe less GPU memory being used during training, explain why in your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbyKZiTgahSv"
   },
   "source": [
    "## üîÆ Model Prediction\n",
    "Below we will use our model to generate text sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DxQ13QcIjPE9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outright <unk> <unk> <unk> so a western spokesman in new york \r\n",
      "rogers got N aliens and instructions have suffered their office \r\n",
      "and it has big early terms with momentum \r\n",
      "we know that he will his next two games needs and credible while our political perspective \r\n",
      "the "
     ]
    }
   ],
   "source": [
    "# Sample from the model\n",
    "with torch.no_grad():\n",
    "    with open('sample.txt', 'w') as f:\n",
    "        # Set intial hidden ane cell states\n",
    "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
    "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
    "\n",
    "        # Select one word id randomly\n",
    "        prob = torch.ones(vocab_size)\n",
    "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Forward propagate RNN \n",
    "            output, state = model(input, state)\n",
    "\n",
    "            # Sample a word id\n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "\n",
    "            # Fill input with sampled word id for the next time step\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # File write\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))\n",
    "! cat sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXsUDt0tbAHM"
   },
   "source": [
    "## 2Ô∏è‚É£ Q2.2 Sampling strategy (7 points)\n",
    "Consider the sampling procedure above. The current code samples a word:\n",
    "```python\n",
    "word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "```\n",
    "in order to feed the model at each output step and feeding those to the next timestep. Copy below the above cell and modify this sampling startegy to use a greedy sampling which selects the highest probability word at each time step to feed as the next input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2BeO7LSWiyIZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter that the leadership change reflects the high of all this \r\n",
      "the <unk> of the <unk> mr. jones has turned <unk> to the <unk> \r\n",
      "mr. roman also raised pinkerton 's equity investment in the u.s. \r\n",
      "he also told reporters that he would n't elaborate \r\n",
      "he would "
     ]
    }
   ],
   "source": [
    "# Sample greedily from the model\n",
    "# Sample from the model\n",
    "with torch.no_grad():\n",
    "    with open('sample.txt', 'w') as f:\n",
    "        # Set intial hidden ane cell states\n",
    "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
    "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
    "\n",
    "        # Select one word id randomly\n",
    "        prob = torch.ones(vocab_size)\n",
    "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Forward propagate RNN \n",
    "            output, state = model(input, state)\n",
    "\n",
    "            # Sample a word id\n",
    "            prob = output.exp()\n",
    "            word_id = torch.argmax(prob, dim=1).item()\n",
    "\n",
    "            # Fill input with sampled word id for the next time step\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # File write\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))\n",
    "! cat sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8YV7laBe9er"
   },
   "source": [
    "## 3Ô∏è‚É£ Q2.3 Embedding Distance (8 points)\n",
    "Our model has learned a specific set of word embeddings.\n",
    "* Write a function that takes in 2 words and prints the cosine distance between their embeddings using the word embeddings from the above models.\n",
    "* Use it to print the cosine distance of the word \"army\" and the word \"taxpayer\".\n",
    "\n",
    "*Refer to the sampling code for how to output the words corresponding to each index. To get the index you can use the function `corpus.dictionary.word2idx.`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "e6w3JSY3d_6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine distance between 'army' and 'taxpayer' is 0.9354\n"
     ]
    }
   ],
   "source": [
    "# Embedding distance\n",
    "def cosine_distance(model, word1, word2):\n",
    "    idx1 = corpus.dictionary.word2idx[word1]\n",
    "    idx2 = corpus.dictionary.word2idx[word2]\n",
    "\n",
    "    # Retrieve the embeddings\n",
    "    embed1 = model.embed(torch.tensor([idx1]).to(device)).squeeze()\n",
    "    embed2 = model.embed(torch.tensor([idx2]).to(device)).squeeze()\n",
    "\n",
    "    # Normalize the embeddings\n",
    "    norm1 = embed1 / torch.norm(embed1)\n",
    "    norm2 = embed2 / torch.norm(embed2)\n",
    "\n",
    "    # Compute the cosine distance (dot product of normalized embeddings)\n",
    "    distance = torch.dot(norm1, norm2).item()\n",
    "\n",
    "    return 1 - distance\n",
    "\n",
    "word1 = \"army\"\n",
    "word2 = \"taxpayer\"\n",
    "distance = cosine_distance(model, word1, word2)\n",
    "print(f\"The cosine distance between '{word1}' and '{word2}' is {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O44EBrsQdA4n"
   },
   "source": [
    "## 4Ô∏è‚É£ Q2.4 Teacher Forcing (Extra Credit 2 points)\n",
    "What is teacher forcing?\n",
    "> Teacher forcing works by using the actual or expected output from the training dataset at the current time step $y(t)$ as input in the next time step $X(t+1)$, rather than the output generated by the network.\n",
    "\n",
    "In the `üèì Training` code this is achieved, implicitly, when we pass the entire input sequence (`inputs = ids[:, i:i+seq_length].to(device)`) to the model at once.\n",
    "\n",
    "Copy below the `üèì Training` code and modify it to disable teacher forcing training. Compare the performance of this model, to original model, what can you conclude? (compare perplexity and convergence rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "qfgf5pJGfL-D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step[0/1549], Loss: 6.6925, Perplexity: 806.36\n",
      "Epoch [1/5], Step[100/1549], Loss: 6.5009, Perplexity: 665.77\n",
      "Epoch [1/5], Step[200/1549], Loss: 6.5244, Perplexity: 681.54\n",
      "Epoch [1/5], Step[300/1549], Loss: 6.6948, Perplexity: 808.16\n",
      "Epoch [1/5], Step[400/1549], Loss: 6.5107, Perplexity: 672.32\n",
      "Epoch [1/5], Step[500/1549], Loss: 6.3665, Perplexity: 582.00\n",
      "Epoch [1/5], Step[600/1549], Loss: 6.3464, Perplexity: 570.43\n",
      "Epoch [1/5], Step[700/1549], Loss: 6.5332, Perplexity: 687.59\n",
      "Epoch [1/5], Step[800/1549], Loss: 6.3586, Perplexity: 577.42\n",
      "Epoch [1/5], Step[900/1549], Loss: 6.4413, Perplexity: 627.23\n",
      "Epoch [1/5], Step[1000/1549], Loss: 6.4862, Perplexity: 656.02\n",
      "Epoch [1/5], Step[1100/1549], Loss: 6.5744, Perplexity: 716.54\n",
      "Epoch [1/5], Step[1200/1549], Loss: 6.4850, Perplexity: 655.22\n",
      "Epoch [1/5], Step[1300/1549], Loss: 6.6614, Perplexity: 781.61\n",
      "Epoch [1/5], Step[1400/1549], Loss: 6.4868, Perplexity: 656.39\n",
      "Epoch [1/5], Step[1500/1549], Loss: 6.4544, Perplexity: 635.51\n",
      "Epoch [2/5], Step[0/1549], Loss: 6.4039, Perplexity: 604.21\n",
      "Epoch [2/5], Step[100/1549], Loss: 6.1875, Perplexity: 486.63\n",
      "Epoch [2/5], Step[200/1549], Loss: 6.3588, Perplexity: 577.56\n",
      "Epoch [2/5], Step[300/1549], Loss: 6.5496, Perplexity: 698.93\n",
      "Epoch [2/5], Step[400/1549], Loss: 6.4144, Perplexity: 610.59\n",
      "Epoch [2/5], Step[500/1549], Loss: 6.2568, Perplexity: 521.53\n",
      "Epoch [2/5], Step[600/1549], Loss: 6.2490, Perplexity: 517.51\n",
      "Epoch [2/5], Step[700/1549], Loss: 6.4943, Perplexity: 661.36\n",
      "Epoch [2/5], Step[800/1549], Loss: 6.2322, Perplexity: 508.90\n",
      "Epoch [2/5], Step[900/1549], Loss: 6.4288, Perplexity: 619.42\n",
      "Epoch [2/5], Step[1000/1549], Loss: 6.4199, Perplexity: 613.95\n",
      "Epoch [2/5], Step[1100/1549], Loss: 6.4798, Perplexity: 651.84\n",
      "Epoch [2/5], Step[1200/1549], Loss: 6.3701, Perplexity: 584.11\n",
      "Epoch [2/5], Step[1300/1549], Loss: 6.5265, Perplexity: 683.02\n",
      "Epoch [2/5], Step[1400/1549], Loss: 6.3400, Perplexity: 566.80\n",
      "Epoch [2/5], Step[1500/1549], Loss: 6.2940, Perplexity: 541.32\n",
      "Epoch [3/5], Step[0/1549], Loss: 6.3361, Perplexity: 564.60\n",
      "Epoch [3/5], Step[100/1549], Loss: 6.0814, Perplexity: 437.65\n",
      "Epoch [3/5], Step[200/1549], Loss: 6.3158, Perplexity: 553.24\n",
      "Epoch [3/5], Step[300/1549], Loss: 6.4307, Perplexity: 620.60\n",
      "Epoch [3/5], Step[400/1549], Loss: 6.3100, Perplexity: 550.07\n",
      "Epoch [3/5], Step[500/1549], Loss: 6.1807, Perplexity: 483.35\n",
      "Epoch [3/5], Step[600/1549], Loss: 6.1920, Perplexity: 488.82\n",
      "Epoch [3/5], Step[700/1549], Loss: 6.3868, Perplexity: 593.96\n",
      "Epoch [3/5], Step[800/1549], Loss: 6.1156, Perplexity: 452.86\n",
      "Epoch [3/5], Step[900/1549], Loss: 6.3059, Perplexity: 547.80\n",
      "Epoch [3/5], Step[1000/1549], Loss: 6.3361, Perplexity: 564.56\n",
      "Epoch [3/5], Step[1100/1549], Loss: 6.3867, Perplexity: 593.90\n",
      "Epoch [3/5], Step[1200/1549], Loss: 6.2883, Perplexity: 538.25\n",
      "Epoch [3/5], Step[1300/1549], Loss: 6.4212, Perplexity: 614.77\n",
      "Epoch [3/5], Step[1400/1549], Loss: 6.3092, Perplexity: 549.61\n",
      "Epoch [3/5], Step[1500/1549], Loss: 6.1814, Perplexity: 483.65\n",
      "Epoch [4/5], Step[0/1549], Loss: 6.2447, Perplexity: 515.28\n",
      "Epoch [4/5], Step[100/1549], Loss: 6.0225, Perplexity: 412.61\n",
      "Epoch [4/5], Step[200/1549], Loss: 6.2967, Perplexity: 542.80\n",
      "Epoch [4/5], Step[300/1549], Loss: 6.3786, Perplexity: 589.09\n",
      "Epoch [4/5], Step[400/1549], Loss: 6.2813, Perplexity: 534.49\n",
      "Epoch [4/5], Step[500/1549], Loss: 6.1486, Perplexity: 468.06\n",
      "Epoch [4/5], Step[600/1549], Loss: 6.1711, Perplexity: 478.69\n",
      "Epoch [4/5], Step[700/1549], Loss: 6.3145, Perplexity: 552.55\n",
      "Epoch [4/5], Step[800/1549], Loss: 6.0381, Perplexity: 419.08\n",
      "Epoch [4/5], Step[900/1549], Loss: 6.2771, Perplexity: 532.27\n",
      "Epoch [4/5], Step[1000/1549], Loss: 6.2572, Perplexity: 521.77\n",
      "Epoch [4/5], Step[1100/1549], Loss: 6.2865, Perplexity: 537.25\n",
      "Epoch [4/5], Step[1200/1549], Loss: 6.2059, Perplexity: 495.64\n",
      "Epoch [4/5], Step[1300/1549], Loss: 6.3128, Perplexity: 551.57\n",
      "Epoch [4/5], Step[1400/1549], Loss: 6.2084, Perplexity: 496.92\n",
      "Epoch [4/5], Step[1500/1549], Loss: 6.1116, Perplexity: 451.08\n",
      "Epoch [5/5], Step[0/1549], Loss: 6.2378, Perplexity: 511.73\n",
      "Epoch [5/5], Step[100/1549], Loss: 5.9888, Perplexity: 398.93\n",
      "Epoch [5/5], Step[200/1549], Loss: 6.2179, Perplexity: 501.66\n",
      "Epoch [5/5], Step[300/1549], Loss: 6.3315, Perplexity: 562.02\n",
      "Epoch [5/5], Step[400/1549], Loss: 6.2712, Perplexity: 529.10\n",
      "Epoch [5/5], Step[500/1549], Loss: 6.0086, Perplexity: 406.90\n",
      "Epoch [5/5], Step[600/1549], Loss: 6.1346, Perplexity: 461.54\n",
      "Epoch [5/5], Step[700/1549], Loss: 6.2731, Perplexity: 530.11\n",
      "Epoch [5/5], Step[800/1549], Loss: 5.9667, Perplexity: 390.24\n",
      "Epoch [5/5], Step[900/1549], Loss: 6.2845, Perplexity: 536.17\n",
      "Epoch [5/5], Step[1000/1549], Loss: 6.2624, Perplexity: 524.50\n",
      "Epoch [5/5], Step[1100/1549], Loss: 6.1798, Perplexity: 482.90\n",
      "Epoch [5/5], Step[1200/1549], Loss: 6.2152, Perplexity: 500.32\n",
      "Epoch [5/5], Step[1300/1549], Loss: 6.2706, Perplexity: 528.79\n",
      "Epoch [5/5], Step[1400/1549], Loss: 6.1707, Perplexity: 478.54\n",
      "Epoch [5/5], Step[1500/1549], Loss: 6.1911, Perplexity: 488.38\n"
     ]
    }
   ],
   "source": [
    "# Training code without Teacher Forcing\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+1].to(device)  # only the first word\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Initialize loss\n",
    "        loss = 0\n",
    "        \n",
    "        for j in range(seq_length):\n",
    "            # Forward pass\n",
    "            states = detach(states)\n",
    "            outputs, states = model(inputs, states)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss += criterion(outputs, targets[:, j])\n",
    "\n",
    "            # Update input for the next step\n",
    "            _, predicted = outputs.max(1)\n",
    "            inputs = predicted.unsqueeze(1).detach()\n",
    "        \n",
    "        # Normalize loss by sequence length\n",
    "        loss /= seq_length\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion for Q2.4\n",
    "Compared with the original model, the model trained without teacher forcing presents a slower convergence rate and higher perplexity values. This is because, without teacher forcing, the model relies on its own generated outputs to learn, which might not be accurate, especially in the early stages of training. As a result, the model might take more time to learn the correct patterns and dependencies in the data. In some cases, it could lead to a more robust model that can better handle unexpected inputs during inference, but it generally takes longer to train and may not achieve the same performance as the model trained with teacher forcing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH2TG4v3PBOu"
   },
   "source": [
    "## 5Ô∏è‚É£ Q2.5 Distance Comparison (+1 point)\n",
    "Repeat the work you did for `3Ô∏è‚É£ Q2.3 Embedding Distance` for the model in `4Ô∏è‚É£ Q2.4 Teacher Forcing` and compare the distances produced by these two models (i.e. with and without the teacher forcing), what can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "EABSoOAGPAaS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine distance between 'army' and 'taxpayer' without teacher forcing is 0.9480\n"
     ]
    }
   ],
   "source": [
    "word1 = \"army\"\n",
    "word2 = \"taxpayer\"\n",
    "distance_without_tf = cosine_distance(model, word1, word2)\n",
    "print(f\"The cosine distance between '{word1}' and '{word2}' without teacher forcing is {distance_without_tf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion for Q2.5\n",
    "The cosine distances produced by the models with and without teacher forcing are different due to the differences in training strategies. They have different learned representations. However, different final embedding layers may not be sufficient to draw conclusions about the overall quality or performance of the models. We still need to consider evaluating both models on a validation dataset and comparing their perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
